import os
import base64
import markdown
import json
from email.mime.text import MIMEText
from urllib.parse import urljoin, urlparse
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
import feedparser
from bs4 import BeautifulSoup
from jinja2 import Environment, FileSystemLoader
from datetime import datetime
import requests

def generate_ai_briefing(news_list):
    print("AI ì„œì‹í™” ë¸Œë¦¬í•‘ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    try:
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        news_context = ""
        for news in news_list:
            news_context += f"ì œëª©: {news['title']}\nìš”ì•½: {news['summary']}\n\n"
        prompt = f"""
        ë‹¹ì‹ ì€ íƒì›”í•œ í†µì°°ë ¥ì„ ê°€ì§„ IT/ê²½ì œ ë‰´ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤.
        ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ë…ìë¥¼ ìœ„í•œ ë§¤ìš° ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ 'ë°ì¼ë¦¬ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        **ì¶œë ¥ í˜•ì‹ ê·œì¹™:**
        1. 'ì—ë””í„° ë¸Œë¦¬í•‘'ì€ '## ì—ë””í„° ë¸Œë¦¬í•‘' í—¤ë”ë¡œ ì‹œì‘í•˜ë©°, ì˜¤ëŠ˜ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        2. 'ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„'ì€ '## ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„' í—¤ë”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        3. ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„ì—ì„œëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ 2~3ê°œë¥¼ '###' í—¤ë”ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
        4. ê° ì¹´í…Œê³ ë¦¬ ì•ˆì—ì„œëŠ”, ê´€ë ¨ëœ ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ **í•˜ë‚˜ì˜ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½**í•˜ê³  ê¸€ë¨¸ë¦¬ ê¸°í˜¸(`*`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        5. ë¬¸ì¥ ì•ˆì—ì„œ ê°•ì¡°í•˜ê³  ì‹¶ì€ íŠ¹ì • í‚¤ì›Œë“œëŠ” êµµì€ ê¸€ì”¨ ëŒ€ì‹  **í°ë”°ì˜´í‘œ(" ")**ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”.

        [ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ëª©ë¡]
        {news_context}
        """
        response = model.generate_content(prompt)
        print("AI ì„œì‹í™” ë¸Œë¦¬í•‘ ìƒì„± ì„±ê³µ!")
        return response.text
    except Exception as e:
        print(f"AI ë¸Œë¦¬í•‘ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def get_image_from_url(page_url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            image_url = og_image['content']
            if image_url.startswith('/'):
                parsed_uri = urlparse(page_url)
                base_url = f"{parsed_uri.scheme}://{parsed_uri.netloc}"
                image_url = urljoin(base_url, image_url)
            return image_url
    except Exception as e:
        print(f"ì´ë¯¸ì§€ URL ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (URL: {page_url}): {e}")
    return None

def update_sent_links(links):
    try:
        with open('sent_links.txt', 'a', encoding='utf-8') as f:
            for link in links:
                f.write(link + '\n')
        print(f"{len(links)}ê°œì˜ ìƒˆ ë§í¬ë¥¼ sent_links.txtì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"sent_links.txt íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- ìƒˆë¡œ ì¶”ê°€ëœ AI ë‰´ìŠ¤ ì„ ë³„ í•¨ìˆ˜ ---
def select_top_news_with_ai(news_list):
    """
    ì „ì²´ ë‰´ìŠ¤ ëª©ë¡ì„ Gemini APIì— ë³´ë‚´ ê°€ì¥ ì¤‘ìš”í•œ Top 10 ë‰´ìŠ¤ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.
    """
    print(f"AI ë‰´ìŠ¤ íë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ëŒ€ìƒ: {len(news_list)}ê°œ)")
    try:
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return news_list[:10] # AIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì• 10ê°œë¥¼ ë°˜í™˜

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')

        # AIì—ê²Œ ì „ë‹¬í•  ë‰´ìŠ¤ ì •ë³´ë¥¼ ê°€ê³µí•©ë‹ˆë‹¤.
        news_context_for_selection = ""
        for i, news in enumerate(news_list):
            news_context_for_selection += f"ê¸°ì‚¬ #{i}:\nì œëª©: {news['title']}\nìš”ì•½: {news['summary']}\n\n"

        # AIì—ê²Œ ë‰´ìŠ¤ ì„ ë³„ì„ ì§€ì‹œí•˜ëŠ” í”„ë¡¬í”„íŠ¸
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ì˜ IT/ê²½ì œ ë¶„ì•¼ë¥¼ ë‹¤ë£¨ëŠ” ì „ë¬¸ ë‰´ìŠ¤ í¸ì§‘ì¥ì…ë‹ˆë‹¤.
        ì•„ë˜ëŠ” ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì…ë‹ˆë‹¤. ê° ê¸°ì‚¬ì—ëŠ” ê³ ìœ í•œ ë²ˆí˜¸(#)ê°€ ìˆìŠµë‹ˆë‹¤.

        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì´ ëª¨ë“  ê¸°ì‚¬ë¥¼ ê²€í† í•˜ì—¬, ì˜¤ëŠ˜ ë…ìë“¤ì´ ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  **ê°€ì¥ ì¤‘ìš”í•˜ê³  ì˜í–¥ë ¥ ìˆëŠ” ê¸°ì‚¬ 10ê°œ**ë¥¼ ì„ ë³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        ì‹œì¥ ë™í–¥, ê¸°ìˆ  í˜ì‹ , ì£¼ìš” ê¸°ì—… ì†Œì‹ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”.

        **ì¶œë ¥ í˜•ì‹ ê·œì¹™:**
        - ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
        - JSON ê°ì²´ëŠ” 'top_10_indices'ë¼ëŠ” í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
        - 'top_10_indices'ì˜ ê°’ì€ ë‹¹ì‹ ì´ ì„ íƒí•œ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì‚¬ 10ê°œì˜ 'ë²ˆí˜¸'ë¥¼ ë‹´ì€ ìˆ«ì ë°°ì—´(array)ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: [3, 15, 4, ...].

        [ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ëª©ë¡]
        {news_context_for_selection}
        """

        response = model.generate_content(prompt)
        # AIì˜ ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        json_response_text = response.text.strip().replace("```json", "").replace("```", "")
        selected_data = json.loads(json_response_text)
        
        selected_indices = selected_data.get('top_10_indices', [])
        
        # ì„ íƒëœ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ë§Œ ê³¨ë¼ì„œ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        top_10_news = [news_list[i] for i in selected_indices if i < len(news_list)]

        print(f"AIê°€ {len(top_10_news)}ê°œì˜ Top ë‰´ìŠ¤ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")
        return top_10_news

    except Exception as e:
        print(f"AI ë‰´ìŠ¤ íë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì‹œìŠ¤í…œì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ ê·¸ëƒ¥ ì• 10ê°œ ë‰´ìŠ¤ë¥¼ ë°˜í™˜
        return news_list[:10]

def get_news_from_rss():
    sent_links = set()
    try:
        with open('sent_links.txt', 'r', encoding='utf-8') as f:
            sent_links = set(line.strip() for line in f)
        print(f"ì´ {len(sent_links)}ê°œì˜ ë³´ë‚¸ ê¸°ë¡ì„ sent_links.txtì—ì„œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print("sent_links.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´, ìƒˆë¡œìš´ ê¸°ë¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    rss_feeds = [
        'https://www.zdnet.co.kr/rss/all.xml', 'https://www.etnews.com/rss/all.xml',
        'https://www.itworld.co.kr/rss', 'https://www.ciokorea.com/rss',
        'https://www.bloter.net/rss', 'http://www.ddaily.co.kr/rss.xml',
        'https://www.hankyung.com/feed/it', 'https://www.mk.co.kr/rss/all.xml',
        'https://rss.mt.co.kr/mt_all.xml', 'https://news.einfomax.co.kr/rss/clickTop.xml',
        'https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml',
        'https://rss.joins.com/joins_news_list.xml', 'https://rss.donga.com/total.xml',
        'https://www.hani.co.kr/rss'
    ]
    
    found_news = []
    unique_links = set()
    print("RSS í”¼ë“œë¥¼ í†µí•´ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for url in rss_feeds:
        try:
            feed = feedparser.parse(url, agent='Mozilla/5.0')
            for entry in feed.entries:
                if entry.link in sent_links or entry.link in unique_links:
                    continue
                
                summary_html = entry.get('summary', 'ìš”ì•½ ì—†ìŒ')
                soup = BeautifulSoup(summary_html, 'lxml')
                summary_text = soup.get_text(strip=True)
                
                keywords = [
                    'AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'LLM', 'ìƒì„±í˜•', 'ChatGPT', 'Gemini', 
                    'AIë°˜ë„ì²´', 'HBM', 'CXL', 'ì£¼ì‹', 'ì¦ì‹œ', 'ì½”ìŠ¤í”¼', 'ë‚˜ìŠ¤ë‹¥', 'ê¸ˆë¦¬', 
                    'í™˜ìœ¨', 'ì‹¤ì ', 'íˆ¬ì', 'M&A', 'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'ì—”ë¹„ë””ì•„', 
                    'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤', 'êµ¬ê¸€', 'ì• í”Œ', 'MS', 'í´ë¼ìš°ë“œ', 'ë°ì´í„°', 'ë¹…ë°ì´í„°'
                ]
                search_text = entry.title + " " + summary_text
                
                for keyword in keywords:
                    if keyword.lower() in search_text.lower():
                        image_url = get_image_from_url(entry.link)
                        news_item = {
                            'title': entry.title,
                            'link': entry.link,
                            'summary': summary_text[:150] + '...',
                            'image_url': image_url
                        }
                        found_news.append(news_item)
                        unique_links.add(entry.link)
                        break
        except Exception as e:
            print(f"'{url}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print(f"ì´ {len(found_news)}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    return found_news

def create_email_html(news_list, ai_briefing):
    env = Environment(loader=FileSystemLoader('.'))
    template = env.get_template('email_template.html')
    today_date = datetime.now().strftime("%Y-%m-%d")
    return template.render(news_list=news_list, today_date=today_date, ai_briefing=ai_briefing)

def send_email_oauth(sender_email, receiver_emails, subject, body):
    SCOPES = ['https://www.googleapis.com/auth/gmail.send']
    creds = None
    if os.path.exists('token.json'):
        creds = Credentials.from_authorized_user_file('token.json', SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.json', 'w') as token:
            token.write(creds.to_json())
    try:
        service = build('gmail', 'v1', credentials=creds)
        message = MIMEText(body, 'html')
        message['To'] = ", ".join(receiver_emails)
        message['From'] = sender_email
        message['Subject'] = subject
        encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
        create_message = {'raw': encoded_message}
        send_message = (service.users().messages().send(userId="me", body=create_message).execute())
        print(f"ë©”ì‹œì§€ ID: {send_message['id']} ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! (ìˆ˜ì‹ ì: {', '.join(receiver_emails)})")
    except HttpError as error:
        print(f"ì´ë©”ì¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")

# --- ìƒˆë¡œ ì¶”ê°€ëœ ìŠ¬ë™ ë©”ì‹œì§€ ë°œì†¡ í•¨ìˆ˜ ---
def send_to_slack(webhook_url, news_list, ai_briefing):
    """
    ë‰´ìŠ¤ë ˆí„° ë‚´ìš©ì„ Slackì˜ Block Kit í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ Webhookìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    if not webhook_url:
        print("ìŠ¬ë™ Webhook URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("ìŠ¬ë™ìœ¼ë¡œ ë‰´ìŠ¤ë ˆí„° ë°œì†¡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ìŠ¬ë™ ë©”ì‹œì§€ í—¤ë”
    today_str = datetime.now().strftime("%Y-%m-%d")
    header_text = f"ğŸ“° ì˜¤ëŠ˜ì˜ AI/ì£¼ì‹/ë¨¸ì‹ ëŸ¬ë‹ Top {len(news_list)} ë‰´ìŠ¤ ({today_str})"
    
    # ìŠ¬ë™ ë©”ì‹œì§€ ë³¸ë¬¸(ë¸”ë¡) êµ¬ì„±
    blocks = [
        {"type": "header", "text": {"type": "plain_text", "text": header_text, "emoji": True}},
    ]
    
    # AI ë¸Œë¦¬í•‘ì´ ìˆìœ¼ë©´ ì¶”ê°€
    if ai_briefing:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": f"*ğŸ¤– ì˜¤ëŠ˜ì˜ ë¸Œë¦¬í•‘*\n{ai_briefing}"}})
        blocks.append({"type": "divider"})

    # ë‰´ìŠ¤ ëª©ë¡ ì¶”ê°€
    for news in news_list:
        news_block = {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*<{news['link']}|{news['title']}>*\n{news['summary']}"
            }
        }
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì¸ë„¤ì¼ë¡œ ì¶”ê°€
        if news.get('image_url'):
            news_block["accessory"] = {
                "type": "image",
                "image_url": news['image_url'],
                "alt_text": "Article thumbnail"
            }
        blocks.append(news_block)
        blocks.append({"type": "divider"})

    payload = {"blocks": blocks}

    try:
        response = requests.post(webhook_url, data=json.dumps(payload), headers={'Content-Type': 'application/json'})
        response.raise_for_status()
        print("ìŠ¬ë™ ë©”ì‹œì§€ ë°œì†¡ ì„±ê³µ!")
    except Exception as e:
        print(f"ìŠ¬ë™ ë©”ì‹œì§€ ë°œì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    recipients_str = os.getenv('RECIPIENT_LIST', 'rjh@ylp.co.kr')
    recipient_list = [email.strip() for email in recipients_str.split(',')]
    SENDER_EMAIL = "zzzfbwnsgh@gmail.com"
    SLACK_WEBHOOK_URL = os.getenv('SLACK_WEBHOOK_URL') # ìŠ¬ë™ URL ë¶ˆëŸ¬ì˜¤ê¸°
    
    # 1. ì¼ë‹¨ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    all_news_data = get_news_from_rss()
    
    if all_news_data:
        # 2. AIë¥¼ ì‚¬ìš©í•´ Top 10 ë‰´ìŠ¤ë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.
        top_news_data = select_top_news_with_ai(all_news_data)

        # 3. ì„ ë³„ëœ Top 10 ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ AI ë¸Œë¦¬í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ai_briefing_markdown = generate_ai_briefing(top_news_data)
        ai_briefing_html = markdown.markdown(ai_briefing_markdown) if ai_briefing_markdown else None
        
        # 4. ìµœì¢… 10ê°œì˜ ë‰´ìŠ¤ì™€ ë¸Œë¦¬í•‘ìœ¼ë¡œ ì´ë©”ì¼ ë³¸ë¬¸ì„ ë§Œë“­ë‹ˆë‹¤.
        email_body = create_email_html(top_news_data, ai_briefing_html)
        email_subject = f"[{datetime.now().strftime('%Y-%m-%d')}] ì˜¤ëŠ˜ì˜ AI/ì£¼ì‹/ë¨¸ì‹ ëŸ¬ë‹ Top 10 ë‰´ìŠ¤"
        send_email_oauth(SENDER_EMAIL, recipient_list, email_subject, email_body)

        # ìŠ¬ë™ ë°œì†¡ (ë§ˆí¬ë‹¤ìš´ ì›ë³¸ì„ ì „ë‹¬)
        send_to_slack(SLACK_WEBHOOK_URL, top_news_data, ai_briefing_markdown)
        
        # 5. ë°œì†¡ëœ 10ê°œ ë‰´ìŠ¤ì˜ ë§í¬ë§Œ ê¸°ë¡í•©ë‹ˆë‹¤.
        new_links_to_save = [news['link'] for news in top_news_data]
        update_sent_links(new_links_to_save)
    else:
        print("ë°œì†¡í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")



